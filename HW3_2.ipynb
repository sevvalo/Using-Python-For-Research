{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "BST263",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "HW3-2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvv2CtryU-3N"
      },
      "source": [
        "# Using Python for Research Homework: Week 3, Case Study 2\n",
        "\n",
        "In this case study, we will find and plot the distribution of word frequencies for each translation of Hamlet.  Perhaps the distribution of word frequencies of Hamlet depends on the translation --- let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LPmk2qU-3a"
      },
      "source": [
        "# DO NOT EDIT THIS CODE!\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def count_words_fast(text):\n",
        "    text = text.lower()\n",
        "    skips = [\".\", \",\", \";\", \":\", \"'\", '\"', \"\\n\", \"!\", \"?\", \"(\", \")\"]\n",
        "    for ch in skips:\n",
        "        text = text.replace(ch, \"\")\n",
        "    word_counts = Counter(text.split(\" \"))\n",
        "    return word_counts\n",
        "\n",
        "def word_stats(word_counts):\n",
        "    num_unique = len(word_counts)\n",
        "    counts = word_counts.values()\n",
        "    return (num_unique, counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ6m-bu1U-3d"
      },
      "source": [
        "### Exercise 1 \n",
        "\n",
        "In this case study, we will find and visualize summary statistics of the text of different translations of Hamlet. For this case study, functions `count_words_fast` and `word_stats` are already defined as in the Case 2 Videos (Videos 3.2.x).\n",
        "\n",
        "#### Instructions \n",
        "- Read in the data as a pandas dataframe using `pd.read_csv`. Use the `index_col` argument to set the first column in the csv file as the index for the dataframe. The data can be found at https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@hamlets.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV7dbONVU-3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "a3bb96d8-dfc9-4545-d35b-b127ba4d40aa"
      },
      "source": [
        "hamlets = pd.read_csv(\"hamlets.csv\", index_col = 1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-790510e3ad3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhamlets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hamlets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hamlets.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_DrVuGzU-3g"
      },
      "source": [
        "### Exercise 2 \n",
        "\n",
        "In this exercise, we will summarize the text for a single translation of Hamlet in a `pandas` dataframe. \n",
        "\n",
        "#### Instructions\n",
        "- Find the dictionary of word frequency in `text` by calling `count_words_fast()`. Store this as `counted_text`.\n",
        "- Create a `pandas` dataframe named `data`.\n",
        "- Using `counted_text`, define two columns in data:\n",
        "    - `word`, consisting of each unique word in text.\n",
        "    - `count`, consisting of the number of times each word in `word` is included in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqqs11JpU-3h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "25d8820b-2b27-4911-cd0e-017b7316a675"
      },
      "source": [
        "language, text = hamlets.iloc[0]\n",
        "counted_text = count_words_fast(text)\n",
        "data = pd.DataFrame(columns=('word','count'))\n",
        "word, count = word_stats(counted_text)\n",
        "i = 1\n",
        "while i < len(counted_text):\n",
        "  stats.loc[i] = word, count\n",
        "  i += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8671034e4c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhamlets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcounted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_words_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hamlets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDlPpfxWU-3i"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "In this exercise, we will continue to define summary statistics for a single translation of Hamlet. \n",
        "\n",
        "#### Instructions\n",
        "- Add a column to data named `length`, defined as the length of each word.\n",
        "- Add another column named `frequency`, which is defined as follows for each word in `data`:\n",
        "    - If `count > 10`, `frequency` is \"frequent\".\n",
        "    - If `1 < count <= 10`, `frequency` is \"infrequent\".\n",
        "    - If `count == 1`, `frequency` is \"unique\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVbAK8luU-3j"
      },
      "source": [
        "data = pd.DataFrame(columns=('word','count', \"length\", \"frequency\"))\n",
        "row_num = 1\n",
        "for word in counted_text:\n",
        "    count = counted_text[word]\n",
        "    if count > 10:\n",
        "        frequency = \"frequent\"\n",
        "    elif 1 < count <= 10:\n",
        "        frequency = \"infrequent\"\n",
        "    elif count == 1:\n",
        "        frequency = \"unique\"\n",
        "    data.loc[row_num] = word, count,len(word),frequency\n",
        "    row_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMbXcgWJU-3j"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "In this exercise, we will summarize the statistics in data into a smaller pandas dataframe. \n",
        "\n",
        "#### Instructions \n",
        "- Create a `pandas` dataframe named `sub_data` including the following columns:\n",
        "    - `language`, which is the language of the text (defined in Exercise 2).\n",
        "    - `frequency`, which is a list containing the strings \"frequent\", \"infrequent\", and \"unique\".\n",
        "    - `mean_word_length`, which is the mean word length of each value in frequency.\n",
        "    - `num_words`, which is the total number of words in each frequency category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL_I6wkwU-3k"
      },
      "source": [
        "sub_data = pd.DataFrame(columns=(\"language\",\"frequency\",\"mean_word_length\",\"num_words\"))\n",
        "\n",
        "i = 1\n",
        "for word in counted_text:\n",
        "    count = counted_text[word]\n",
        "    if count > 10:\n",
        "        frequency = \"frequent\"\n",
        "        sum = 0\n",
        "        for j in data.length[data.frequency == \"frequent\"]:\n",
        "            sum += j\n",
        "\n",
        "        mean_word_length = sum / len(data[data.frequency == \"frequent\"])\n",
        "        num_words = len(data[data.frequency == \"frequent\"])\n",
        "    elif 1 < count <= 10:\n",
        "        frequency = \"infrequent\"\n",
        "        sum = 0\n",
        "        for j in data.length[data.frequency == \"infrequent\"]:\n",
        "            sum += j\n",
        "\n",
        "        mean_word_length = sum / len(data[data.frequency == \"infrequent\"])\n",
        "        num_words = len(data[data.frequency == \"infrequent\"])\n",
        "    elif count == 1:\n",
        "        frequency = \"unique\"\n",
        "        sum = 0\n",
        "        for j in data.length[data.frequency == \"unique\"]:\n",
        "            sum += j\n",
        "\n",
        "        mean_word_length = sum / len(data[data.frequency == \"unique\"])\n",
        "        num_words = len(data[data.frequency == \"unique\"])\n",
        "    \n",
        "    sub_data.loc[i] = language, frequency,mean_word_length,num_words\n",
        "    i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "othaEnw0U-3l"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "In this exercise, we will join all the data summaries for text Hamlet translation.\n",
        "\n",
        "#### Instructions \n",
        "- The previous code for summarizing a particular translation of Hamlet is consolidated into a single function called `summarize_text`. Create a pandas dataframe` grouped_data` consisting of the results of `summarize_text` for each translation of Hamlet in `hamlets`.\n",
        "    - Use a `for` loop across the row indices of `hamlets` to assign each translation to a new row.\n",
        "    - Obtain the `ith` row of `hamlets` to variables using the `.iloc` method, and assign the output to variables `language` and `text`.\n",
        "    - Call `summarize_text` using `language` and `text`, and assign the output to `sub_data`.\n",
        "    - Use the pandas `.append()` function to append to pandas dataframes row-wise to `grouped_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj8ukhIeU-3n"
      },
      "source": [
        "def summarize_text(language, text):\n",
        "    counted_text = count_words_fast(text)\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        \"word\": list(counted_text.keys()),\n",
        "        \"count\": list(counted_text.values())\n",
        "    })\n",
        "    \n",
        "    data.loc[data[\"count\"] > 10,  \"frequency\"] = \"frequent\"\n",
        "    data.loc[data[\"count\"] <= 10, \"frequency\"] = \"infrequent\"\n",
        "    data.loc[data[\"count\"] == 1,  \"frequency\"] = \"unique\"\n",
        "    \n",
        "    data[\"length\"] = data[\"word\"].apply(len)\n",
        "    \n",
        "    sub_data = pd.DataFrame({\n",
        "        \"language\": language,\n",
        "        \"frequency\": [\"frequent\",\"infrequent\",\"unique\"],\n",
        "        \"mean_word_length\": data.groupby(by = \"frequency\")[\"length\"].mean(),\n",
        "        \"num_words\": data.groupby(by = \"frequency\").size()\n",
        "    })\n",
        "    \n",
        "    return sub_data\n",
        "grouped_data = pd.DataFrame()\n",
        "for i in range(len(hamlets)+1):\n",
        "  language, text = hamlets.iloc[i]\n",
        "  sub_data = pd.DataFrame(summarize_text(language, text))\n",
        "  grouped_data.append(sub_data)\n",
        "\n",
        "\n",
        "grouped_data.mean_word_length[grouped_data.language == \"German\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekW2EyLfU-3o"
      },
      "source": [
        "### Exercise 6\n",
        "\n",
        "In this exercise, we will plot our results and look for differences across each translation.\n",
        "\n",
        "#### Instructions \n",
        "- Plot the word statistics of each translations on a single plot. Note that we have already done most of the work for you.\n",
        "- Consider: do the word statistics differ by translation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyvX6oH3U-3p"
      },
      "source": [
        "colors = {\"Portuguese\": \"green\", \"English\": \"blue\", \"German\": \"red\"}\n",
        "markers = {\"frequent\": \"o\",\"infrequent\": \"s\", \"unique\": \"^\"}\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(grouped_data.shape[0]):\n",
        "    row = grouped_data.iloc[i]\n",
        "    plt.plot(row.mean_word_length, row.num_words,\n",
        "        marker=markers[row.frequency],\n",
        "        color = colors[row.language],\n",
        "        markersize = 10\n",
        "    )\n",
        "\n",
        "color_legend = []\n",
        "marker_legend = []\n",
        "for color in colors:\n",
        "    color_legend.append(\n",
        "        plt.plot([], [],\n",
        "        color=colors[color],\n",
        "        marker=\"o\",\n",
        "        label = color, markersize = 10, linestyle=\"None\")\n",
        "    )\n",
        "for marker in markers:\n",
        "    marker_legend.append(\n",
        "        plt.plot([], [],\n",
        "        color=\"k\",\n",
        "        marker=markers[marker],\n",
        "        label = marker, markersize = 10, linestyle=\"None\")\n",
        "    )\n",
        "plt.legend(numpoints=1, loc = \"upper left\")\n",
        "\n",
        "plt.xlabel(\"Mean Word Length\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "# write your code to display the plot here!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}